{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Library","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(0)\n\nimport numpy as np\nnp.random.seed(0)\n\nimport tensorflow as tf\ntf.random.set_seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nfrom zipfile import ZipFile\nfrom PIL import Image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers,models\nfrom tensorflow.keras import Model \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explore The Dataset \n","metadata":{}},{"cell_type":"code","source":"dataset_dir = \"/kaggle/input/plantvillage-dataset/color\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### change this current directory to this path,\n","metadata":{}},{"cell_type":"code","source":"cd /kaggle/input/plantvillage-dataset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"plantvillage dataset\"))\n\n\nprint(len(os.listdir(\"plantvillage dataset/segmented\")))\nprint(os.listdir(\"plantvillage dataset/segmented\")[:5])\n\nprint(len(os.listdir(\"plantvillage dataset/color\")))\nprint(os.listdir(\"plantvillage dataset/color\")[:5])\n\nprint(len(os.listdir(\"plantvillage dataset/grayscale\")))\nprint(os.listdir(\"plantvillage dataset/grayscale\")[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir(\"plantvillage dataset/color/Grape___healthy\")))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data preprocessing","metadata":{}},{"cell_type":"code","source":"#Dataset Path\ndataset_dir = \"/kaggle/input/plantvillage-dataset/color\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path='/kaggle/input/plantvillage-dataset/color/Apple___Apple_scab/00075aa8-d81a-4184-8541-b692b78d398a___FREC_Scab 3335.JPG'\nimg=mpimg.imread(image_path)\nprint(img.shape)\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path='/kaggle/input/plantvillage-dataset/grayscale/Apple___Apple_scab/00075aa8-d81a-4184-8541-b692b78d398a___FREC_Scab 3335.JPG'\nimg=mpimg.imread(image_path)\nprint(img.shape)\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path='/kaggle/input/plantvillage-dataset/segmented/Apple___Apple_scab/01a66316-0e98-4d3b-a56f-d78752cd043f___FREC_Scab 3003_final_masked.jpg'\nimg=mpimg.imread(image_path)\nprint(img.shape)\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = '/kaggle/input/plantvillage-dataset/color/Apple___Apple_scab/00075aa8-d81a-4184-8541-b692b78d398a___FREC_Scab 3335.JPG'\n\n# Read the image(Pixels)\nimg = mpimg.imread(image_path)\n\nprint(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Parameters\nimg_size = 224\nbatch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train Test Split","metadata":{}},{"cell_type":"code","source":"# Image Data Generators\ndata_gen=ImageDataGenerator(\nrescale=1./255,\nvalidation_split=0.2, \nrotation_range=0.2,\nhorizontal_flip=0.2,   # Randomly flip images horizontally\nvertical_flip=0.2 \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Generator\ntrain_generator=data_gen.flow_from_directory(\ndataset_dir,\ntarget_size=(img_size,img_size),\nbatch_size=batch_size,\nsubset='training',\nclass_mode='categorical'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_generator=data_gen.flow_from_directory(\ndataset_dir,\ntarget_size=(img_size,img_size),\nbatch_size=batch_size,\nsubset='validation',\nclass_mode='categorical'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional Neural Network","metadata":{}},{"cell_type":"code","source":"model = models.Sequential([\n    layers.Conv2D(32,(3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(train_generator.num_classes, activation='softmax'),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the Model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_generator,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=5,  # Number of epochs\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // batch_size       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation\nprint(\"Evaluating model...\")\nval_loss, val_accuracy = model_final.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Load and Preprocess the Image using Pillow\ndef load_and_preprocess_image(image_path, target_size=(224, 224)):\n    # Load the image\n    img = Image.open(image_path)\n    # Resize the image\n    img = img.resize(target_size)\n    # Convert the image to a numpy array\n    img_array = np.array(img)\n    # Add batch dimension\n    img_array = np.expand_dims(img_array, axis=0)\n    # Scale the image values to [0, 1]\n    img_array = img_array.astype('float32') / 255.\n    return img_array\n\n# Function to Predict the Class of an Image\ndef predict_image_class(model, image_path, class_indices):\n    preprocessed_img = load_and_preprocess_image(image_path)\n    predictions = model.predict(preprocessed_img)\n    predicted_class_index = np.argmax(predictions, axis=1)[0]\n    predicted_class_name = class_indices[predicted_class_index]\n    return predicted_class_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a mapping from class indices to class names\nclass_indices = {v: k for k, v in train_generator.class_indices.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path='/kaggle/input/plantvillage-dataset/color/Strawberry___healthy/00166615-5e7b-4318-8957-5e50df335ee8___RS_HL 1785.JPG'\npredicted_class_name = predict_image_class(model, image_path, class_indices)\nimg=mpimg.imread(image_path)\nplt.imshow(img)\n# Output the result\nprint(\"Predicted Class Name:\", predicted_class_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}